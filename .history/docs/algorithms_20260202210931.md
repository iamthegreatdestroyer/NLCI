# LSH Algorithms in NLCI

This document explains the Locality-Sensitive Hashing (LSH) algorithms and embedding techniques used in NLCI.

## Table of Contents

- [Overview](#overview)
- [Neural Embeddings](#neural-embeddings)
- [LSH Algorithm](#lsh-algorithm)
- [Query Process](#query-process)
- [Parameter Tuning](#parameter-tuning)
- [Performance Analysis](#performance-analysis)

---

## Overview

NLCI uses **LSH (Locality-Sensitive Hashing)** to enable sub-linear similarity search on code embeddings.

### Key Properties

- **Indexing**: O(n) time to index n code blocks
- **Query**: O(1) average time to find similar blocks
- **Memory**: O(n) space for storing hash tables
- **Precision/Recall**: Configurable via L and K parameters

### Architecture

```
Source Code
    ↓
[1] Code Parsing & Chunking
    ↓
[2] Neural Embedding (384-dim)
    ↓
[3] LSH Indexing (L tables × K bits)
    ↓
[4] Bucket Storage
    ↓
[5] Query & Candidate Retrieval
```

---

## Neural Embeddings

### Embedding Model

NLCI uses a transformer-based model to generate semantic embeddings:

- **Architecture**: CodeBERT-based encoder
- **Dimension**: 384
- **Training**: Pre-trained on code corpora
- **Context**: Supports up to 512 tokens

### Embedding Process

```typescript
function embed(code: string): Float32Array {
  // 1. Tokenize
  const tokens = tokenizer.encode(code);
  
  // 2. Forward pass through model
  const output = model.forward(tokens);
  
  // 3. Pool to fixed dimension
  const embedding = meanPooling(output);
  
  // 4. Normalize
  return normalize(embedding);
}
```

### Properties

**Semantic Similarity**:
- Similar code → similar embeddings
- Euclidean distance reflects code similarity

**Normalization**:
- All embeddings normalized to unit length
- Enables use of cosine similarity

---

## LSH Algorithm

### Random Hyperplane Projection

NLCI uses **random hyperplane LSH** for Euclidean space:

#### Algorithm

For each hash table `i` in `1..L`:

1. Generate `K` random hyperplanes: `h_1, h_2, ..., h_K`
2. For each hyperplane `h_j`:
   - `h_j` is a random unit vector in `d` dimensions
3. Hash function: `hash_i(v) = [sign(h_1·v), sign(h_2·v), ..., sign(h_K·v)]`
4. Store `v` in bucket `hash_i(v)`

#### Visualization

```
            h1
            |
    Q2  |   |   Q1
        |   |
--------|---|-------  h2
        |   |
    Q3  |   |   Q4
            |
```

Each hyperplane divides space into 2 half-spaces. K hyperplanes create 2^K regions.

### Hash Function

```typescript
class Hyperplane {
  private vector: Float32Array;  // Random unit vector
  
  hash(embedding: Float32Array): number {
    // Dot product
    let dot = 0;
    for (let i = 0; i < embedding.length; i++) {
      dot += this.vector[i] * embedding[i];
    }
    
    // Return 0 or 1
    return dot >= 0 ? 1 : 0;
  }
}
```

### Multi-table Indexing

```typescript
class LshIndex {
  private tables: HashTable[];  // L hash tables
  
  constructor(config: { numTables: number, numBits: number }) {
    this.tables = Array.from({ length: config.numTables }, () =>
      new HashTable(config.numBits)
    );
  }
  
  insert(id: string, vector: Float32Array): void {
    // Insert into all L tables
    for (const table of this.tables) {
      const hash = table.hash(vector);
      table.insert(hash, id);
    }
  }
}
```

### Collision Probability

The probability that two vectors `u` and `v` collide:

```
P[hash(u) = hash(v)] = 1 - θ/π
```

where `θ` is the angle between `u` and `v`.

For K-bit hashes:

```
P[K-bit collision] = (1 - θ/π)^K
```

For L tables:

```
P[at least 1 collision] = 1 - (1 - (1 - θ/π)^K)^L
```

---

## Query Process

### Step-by-Step

```typescript
async query(queryVector: Float32Array): Promise<string[]> {
  // 1. Hash query vector in all L tables
  const candidates = new Set<string>();
  
  for (const table of this.tables) {
    const hash = table.hash(queryVector);
    const bucket = table.getBucket(hash);
    
    // 2. Collect all candidates from bucket
    for (const id of bucket) {
      candidates.add(id);
    }
  }
  
  // 3. Compute exact similarities
  const results: Array<{id: string, similarity: number}> = [];
  
  for (const id of candidates) {
    const vector = this.getVector(id);
    const similarity = cosineSimilarity(queryVector, vector);
    
    if (similarity >= this.threshold) {
      results.push({ id, similarity });
    }
  }
  
  // 4. Sort by similarity
  results.sort((a, b) => b.similarity - a.similarity);
  
  return results.map(r => r.id);
}
```

### Complexity Analysis

| Phase | Operation | Complexity |
|-------|-----------|------------|
| 1 | Hash computation | O(L × K × d) |
| 2 | Bucket lookup | O(L) |
| 3 | Candidate collection | O(L × B) where B = avg bucket size |
| 4 | Similarity computation | O(C × d) where C = candidates |
| 5 | Sorting | O(C log C) |

**Average case**: O(1) when `C << n`

### Optimization: Early Termination

```typescript
// Stop after finding enough candidates
const maxCandidates = 100;
if (candidates.size >= maxCandidates) break;
```

---

## Parameter Tuning

### L (Number of Tables)

**Effect**: Controls recall

- **Low L (5-10)**: Fast, lower recall
- **Medium L (15-25)**: Balanced
- **High L (30+)**: Slower, higher recall

**Trade-off**: More tables → more memory, slower insertion, higher recall

### K (Bits per Hash)

**Effect**: Controls precision

- **Low K (4-8)**: Large buckets, more false positives
- **Medium K (10-14)**: Balanced
- **High K (16+)**: Small buckets, may miss similar items

**Trade-off**: More bits → smaller buckets, higher precision, lower recall

### Threshold (Similarity)

**Effect**: Filters query results

- **Low (0.7-0.8)**: Includes semantic clones (Type-4)
- **Medium (0.85-0.9)**: Near-miss clones (Type-3)
- **High (0.95+)**: Parameterized clones (Type-2)

### Recommended Presets

| Use Case | L | K | Threshold | Recall | Precision |
|----------|---|---|-----------|--------|-----------|
| Fast Scan | 10 | 8 | 0.85 | 90% | 85% |
| Balanced | 20 | 12 | 0.85 | 95% | 92% |
| High Precision | 30 | 16 | 0.90 | 98% | 97% |

---

## Performance Analysis

### Indexing

**Theoretical**: O(n × L × K)

Where:
- n = number of code blocks
- L = number of tables
- K = bits per hash

**Practical**:
- 10,000 blocks: ~5 seconds
- 100,000 blocks: ~50 seconds

### Query

**Theoretical**: O(1) average

**Practical**:
- Average: 1-5ms
- 99th percentile: 10-20ms

### Memory

**Formula**:

```
Memory = n × (d × 4 bytes)           # Embeddings
       + L × 2^K × 8 bytes          # Hash tables
       + n × metadata_size          # Metadata
```

For default config (L=20, K=12, d=384):

```
Memory ≈ n × (1.5 KB + 80 KB / n + metadata)
       ≈ n × 2 KB per block
```

**Example**:
- 10,000 blocks: ~20 MB
- 100,000 blocks: ~200 MB

### Scalability

| Blocks | Index Time | Query Time | Memory |
|--------|------------|------------|--------|
| 1K | 0.5s | 1ms | 2 MB |
| 10K | 5s | 2ms | 20 MB |
| 100K | 50s | 3ms | 200 MB |
| 1M | 500s | 5ms | 2 GB |

---

## Mathematical Foundations

### Cosine Similarity

For normalized vectors u and v:

```
similarity(u, v) = u · v = Σ u_i × v_i
```

Range: [-1, 1], typically [0, 1] for code embeddings

### Angle Preservation

LSH preserves angles:

```
P[hash(u) = hash(v)] ∝ 1 - angle(u, v) / π
```

This means similar vectors (small angle) have high collision probability.

### False Positive Rate

For threshold τ and angle θ:

```
FPR = P[collision | angle > arccos(τ)]
```

Can be controlled via K and threshold.

### False Negative Rate

```
FNR = P[no collision | angle ≤ arccos(τ)]
```

Can be reduced by increasing L.

---

## Implementation Details

### Random Hyperplane Generation

```typescript
function generateRandomHyperplane(dim: number): Float32Array {
  const vec = new Float32Array(dim);
  
  // Sample from standard normal distribution
  for (let i = 0; i < dim; i++) {
    vec[i] = boxMullerTransform();  // N(0, 1)
  }
  
  // Normalize to unit length
  return normalize(vec);
}
```

### Bucket Storage

```typescript
class BucketStore {
  private buckets: Map<string, Set<string>>;
  
  insert(hash: string, id: string): void {
    if (!this.buckets.has(hash)) {
      this.buckets.set(hash, new Set());
    }
    this.buckets.get(hash)!.add(id);
  }
  
  get(hash: string): Set<string> {
    return this.buckets.get(hash) || new Set();
  }
}
```

---

## References

1. **LSH Original Paper**:
   - Indyk & Motwani (1998): "Approximate nearest neighbors: towards removing the curse of dimensionality"

2. **Random Hyperplane LSH**:
   - Charikar (2002): "Similarity estimation techniques from rounding algorithms"

3. **Code Embeddings**:
   - Feng et al. (2020): "CodeBERT: A Pre-Trained Model for Programming and Natural Languages"

4. **Clone Detection**:
   - Roy & Cordy (2009): "A Survey on Software Clone Detection Research"

---

## Next Steps

- [API Reference](api-reference.md) - Use the algorithms via API
- [Getting Started](getting-started.md) - Quick start guide
- [Examples](../examples/) - Example applications
